{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevin/.pyenv/versions/3.10.2/envs/python_bytes_generator/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"arvkevi/python-bytes-distilgpt2\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"arvkevi/python-bytes-distilgpt2\", pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastapi is a great new web framework to easily build web APIs. So if you want to add a web application or framework and you want to have an async view, that's awesome. So you can create an API using the same API, they have it manage a whole bunch of requests, all sorts of cool stuff, like flowchart, Mock, Adafruit Red, Flask, Kaggle, Flask, Postgres, Postgres, they all have a couple of features that you can leverage.\n",
      " \n",
      "00:01:25 OKKEN: Yeah.\n",
      " \n",
      "00:01:26 KENNEDY: And one of the things I like about them is that they're based on all the underlying logic and so a lot of those kind of have both interfaces, so they take it up some level for a real application. A lot of that's because of some reasons that some of the the ones I want to implement here are just for MongoDB.\n",
      " \n",
      "00:01:42 OKKEN: Yeah, and there's a some other things that we can do to get MongoDB performance better, which I think the super cool thing about this is how you can do things with the fact that it supports HTTP/2 and Celery, Celery runs on multiple CPUs, so that you don't have to do, you get like hundreds or thousands of requests a second with a certain runtime. So I like this idea of having a little bit of a schedule happening all the time. I don't know what exactly is being served that's going on, because of that. I'm not really sure what it's doing, but maybe it'll be a little bit faster than I would otherwise expect, but it's also cool. So if you want to have some an endpoint like I've mentioned to my team. And then with object-oriented APIs, let me say, you're in a modern app or something, with a couple of reasons. Right. Like, the back end part of the query engine is just coming in, and you really do have to ask the server to do some ID. So that's really nice. So you can take the status of the request that's going on, actually start passing the status, and so it'll receive that result, and then maybe the response will break off when you ask that.\n",
      " \n",
      "00:01:51 OKKEN: Okay, and you could maybe have something like a database back end with the person who creates it.\n",
      " \n",
      "00:01:56 KENNEDY: Oh, my gosh, and that can contain possibly multiple IDs. If you take the status message that you're sending, the database will find that you're listening to it's the user ID at the bottom of the request, right. You want that user ID to be the User Id in the object, but if it's not the user ID at the top of the response, you got to do a debug exit in there. So the ORM error message, or other kind of,\n",
      " \n",
      "00:02:00 KENNEDY: Oh, gosh, even if it's just the bottom of it, it's getting thrown up on it, right? Anyway, it looks really simple. So yeah, it's definitely cool to have a framework that you can build very fast and very consistent. So one of the other things I'm looking forward to at I'm gonna bring up is native encoding, which is not only because you have a bunch of operations that you can go back and write to Windows.\n",
      " \n",
      "00:02:02 OKKEN: Oh yeah, that's cool.\n",
      " \n",
      "00:02:03 KENNEDY: That's cool.\n",
      " \n",
      "00:02:04 OKKEN: I want to talk about a library that takes CPU-intensive data sets and gets a lot of RAM. So the ability to deal with that is great, it's fast, so you can basically put it all together, so I thought I wanted to talk about this bit, and it also talks about my workflow on the server, which is pretty cool. I wanted to talk about a good way to do some sort of parsing of your input, and we're mostly using C to do that. So if you're working with c++ code or C++ you've got a lot of data in front of it, and you've got a little c++ image, and you write an HTTP client and send it back. You don't need a bunch of requests, but you can run that on your servers, and those are some like, we might have to do some kind of faulting or something like this in memory, right. So if it's the user, you know, where's the data? There's lots of file, and a bunch of libraries you can write, you can use them, and they don't have as much overhead of, or RAM, but like a lot of them are out there right there.\n",
      " \n",
      "00\n"
     ]
    }
   ],
   "source": [
    "prompt = \"fastapi is a great new web framework to easily build web APIs\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,\n",
    "    max_length=1024,\n",
    "    temperature=1.05,\n",
    "    top_k=0,\n",
    "    top_p=0.90,\n",
    "    repitition_penalty=1.0,\n",
    ")\n",
    "\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=False)\n",
    "print(generated_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"python_bytes_distilgpt2_convo.txt\", \"w\") as f:\n",
    "    f.write(generated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('python_bytes_generator')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d5d1f627fbcc436334a7a88a32991e0bc0b4367c2ad0dd24a00b89bdaddd952a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
